import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# === HYPERPARAMETERS ===
SEQ_LEN = 256       # length of 1D EEG signal sequence
Z_DIM = 100         # latent noise vector size
BATCH_SIZE = 64
LR = 0.0002
EPOCHS = 100
CRITIC_ITER = 5     # discriminator updates per generator update
LAMBDA_GP = 10      # gradient penalty coefficient
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === MODEL DEFINITIONS ===

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(Z_DIM, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            nn.Linear(1024, SEQ_LEN),
            nn.Tanh()  # output range [-1,1], EEG normalized amplitude
        )
    def forward(self, z):
        return self.net(z).unsqueeze(1)  # (batch, channels=1, seq_len)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Flatten(),
            nn.Linear(128 * (SEQ_LEN // 4), 1)
        )
    def forward(self, x):
        return self.net(x).view(-1)

# === HELPER FUNCTIONS ===

def gradient_penalty(critic, real, fake):
    batch_size, _, seq_len = real.shape
    epsilon = torch.rand(batch_size, 1, 1, device=DEVICE).expand_as(real)
    interpolated = (epsilon * real + (1 - epsilon) * fake).requires_grad_(True)
    mixed_scores = critic(interpolated)
    grad_outputs = torch.ones_like(mixed_scores, device=DEVICE)

    gradients = torch.autograd.grad(
        outputs=mixed_scores,
        inputs=interpolated,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients = gradients.view(batch_size, -1)
    grad_norm = gradients.norm(2, dim=1)
    gp = LAMBDA_GP * ((grad_norm - 1) ** 2).mean()
    return gp

def plot_eeg_signals_popup(signals, num=5):
    """
    Pops up an interactive matplotlib window showing `num` EEG signals.
    signals shape: (batch, channels=1, seq_len)
    """
    plt.ion()
    fig, axs = plt.subplots(num, 1, figsize=(12, num * 2))

    if num == 1:
        axs = [axs]

    for i in range(num):
        sig = signals[i].squeeze().cpu().numpy()
        axs[i].plot(sig)
        axs[i].set_title(f"Synthetic EEG Sample #{i+1}")
        axs[i].set_ylim([-1, 1])
        axs[i].set_xlabel("Sample Index")
        axs[i].set_ylabel("Amplitude")

    plt.tight_layout()
    plt.show()
    plt.pause(0.001)  # Ensure window pops up

# === SYNTHETIC REAL EEG DATA GENERATOR ===
# (For demo purposes, sine waves + noise simulating EEG)

def generate_real_eeg_samples(batch_size):
    signals = []
    for _ in range(batch_size):
        t = np.linspace(0, 1, SEQ_LEN)
        freq = np.random.uniform(8, 12)  # Alpha rhythm ~8-12Hz
        signal = 0.5 * np.sin(2 * np.pi * freq * t)
        noise = 0.1 * np.random.randn(SEQ_LEN)
        eeg = signal + noise
        eeg = np.clip(eeg, -1, 1)
        signals.append(eeg)
    signals = np.array(signals).astype(np.float32)
    signals = torch.tensor(signals).unsqueeze(1)  # (batch, 1, seq_len)
    return signals.to(DEVICE)

# === MAIN TRAINING LOOP ===

def train_gan():
    generator = Generator().to(DEVICE)
    discriminator = Discriminator().to(DEVICE)

    opt_gen = optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.9))
    opt_disc = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.9))

    for epoch in range(EPOCHS):
        for _ in range(CRITIC_ITER):
            # Train discriminator
            discriminator.zero_grad()

            real = generate_real_eeg_samples(BATCH_SIZE)
            noise = torch.randn(BATCH_SIZE, Z_DIM, device=DEVICE)
            fake = generator(noise)

            disc_real = discriminator(real)
            disc_fake = discriminator(fake.detach())
            gp = gradient_penalty(discriminator, real, fake.detach())

            loss_disc = -(torch.mean(disc_real) - torch.mean(disc_fake)) + gp
            loss_disc.backward()
            opt_disc.step()

        # Train generator
        generator.zero_grad()
        noise = torch.randn(BATCH_SIZE, Z_DIM, device=DEVICE)
        fake = generator(noise)
        gen_loss = -torch.mean(discriminator(fake))
        gen_loss.backward()
        opt_gen.step()

        # Logging
        if (epoch + 1) % 10 == 0 or epoch == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}] | D loss: {loss_disc.item():.4f} | G loss: {gen_loss.item():.4f}")

            # Popup plot of generated samples
            generator.eval()
            with torch.no_grad():
                sample_noise = torch.randn(5, Z_DIM, device=DEVICE)
                sample_fake = generator(sample_noise)
                plot_eeg_signals_popup(sample_fake, num=5)
            generator.train()

    # Final popup plot after training ends
    print("Training complete! Showing final generated EEG samples...")
    generator.eval()
    with torch.no_grad():
        final_noise = torch.randn(5, Z_DIM, device=DEVICE)
        final_fake = generator(final_noise)
        plot_eeg_signals_popup(final_fake, num=5)

if __name__ == "__main__":
    train_gan()
